# Progress Log
PRD: k8s-resource-requests
Started: 2026-02-16

## Codebase Patterns

**Helm Values Pattern**: For Helm-managed charts, resource requests are added directly to the top-level component keys in `values.yaml`:
```yaml
componentName:
  resources:
    requests:
      cpu: <amount>
      memory: <amount>
```
Common component names: `server`, `repoServer`, `controller`, `redis`, `applicationSet`, `notifications`

**Discovering Helm Chart Structure**: If top-level `resources` doesn't work, use `helm show values <repo>/<chart> --version <version>` to find the correct nested key structure (e.g., `pocketID.resources`, `pipelines.resources`)

**Raw Manifest Pattern**: For raw k8s manifests (non-Helm), add resources in the container spec:
```yaml
containers:
  - name: container-name
    image: ...
    imagePullPolicy: ...
    resources:
      requests:
        cpu: <amount>
        memory: <amount>
    ports: ...  # or readinessProbe/volumeMounts/etc
```
Position: after `imagePullPolicy`, before `ports`/`readinessProbe`/`volumeMounts`

---
<!-- Task logs below - APPEND ONLY -->

## Task - resources-argo-cd
- Added resource requests to all 6 argo-cd components via values.yaml
- Files changed: `/home/devin/Projects/homelab/charts/argo-cd/values.yaml:22-66`
- **Learnings:** 
  - Argo CD Helm chart uses top-level keys for each component (server, repoServer, controller, redis, applicationSet, notifications)
  - Branch naming follows pattern: `k8s-resource-requests/<task-id>`
  - YAML syntax validation passes with python yaml.safe_load

## Task - resources-traefik
- Added resource requests to traefik deployment via values.yaml
- Files changed: `/home/devin/Projects/homelab/charts/traefik/values.yaml:61-64`
- **Learnings:** 
  - Traefik uses a simple top-level `resources` key in values.yaml
  - Verified with `kustomize build --enable-helm` that resources apply correctly to Deployment
  - Resources show in container spec as expected (20m CPU, 256Mi memory)

## Task - resources-homeassistant
- Added resource requests to all 3 homeassistant namespace pods
- Files changed:
  - `/home/devin/Projects/homelab/charts/homeassistant/values.yaml:7-10` (home-assistant: 20m CPU, 1408Mi)
  - `/home/devin/Projects/homelab/charts/homeassistant/esphome.yaml:21-24` (esphome: 10m CPU, 128Mi)
  - `/home/devin/Projects/homelab/charts/homeassistant/zwave.yaml:21-24` (zwavejsui: 20m CPU, 256Mi)
- **Learnings:**
  - homeassistant uses mixed approach: Helm chart for main app, raw manifests for esphome/zwavejsui
  - Raw manifests need resources added in container spec directly (after imagePullPolicy, before ports/readinessProbe)
  - Verified all 3 pods have resources via `kustomize build --enable-helm` output

## Task - resources-postgres
- Added resource requests to postgres deployment and prometheus-postgres-exporter
- Files changed:
  - `/home/devin/Projects/homelab/charts/postgres/manifest.yaml:67-71` (postgres: 10m CPU, 384Mi)
  - `/home/devin/Projects/homelab/charts/postgres/values.yaml:3-6` (postgres-exporter: 10m CPU, 128Mi)
- **Learnings:**
  - postgres uses hybrid: raw manifest for main DB, Helm chart (prometheus-postgres-exporter) for metrics
  - Exporter uses simple top-level `resources` key in values.yaml
  - Verified with `kustomize build --enable-helm` that both pods get resource requests

## Task - resources-open-webui
- Added resource requests to all 3 open-webui namespace pods via values.yaml
- Files changed: `/home/devin/Projects/homelab/charts/open-webui/values.yaml:4-22`
- **Learnings:**
  - open-webui uses Helm chart with top-level `resources`, `pipelines.resources`, and `tika.resources` keys
  - All three pods (open-webui, pipelines, tika) get resources: open-webui (10m, 1024Mi), pipelines (10m, 128Mi), tika (10m, 384Mi)
  - tika already had resource limits; we added requests below them
  - Verified with `kustomize build --enable-helm` that all 3 deployments have resource requests

## Task - resources-pocket-id
- Added resource requests to pocket-id StatefulSet via values.yaml
- Files changed: `/home/devin/Projects/homelab/charts/pocket-id/values.yaml:4-7`
- **Learnings:**
  - pocket-id (anza-labs chart) uses nested `pocketID.resources` key, not top-level `resources`
  - Chart renders as StatefulSet with single pod (10m CPU, 128Mi memory)
  - Used `helm show values` to discover correct values structure when top-level key didn't work
  - Verified with `kustomize build --enable-helm` that resources apply correctly

## Task - resources-dawarich
- Added resource requests to both dawarich namespace pods via values.yaml
- Files changed: `/home/devin/Projects/homelab/charts/dawarich/values.yaml:10-17`
- **Learnings:**
  - dawarich chart (cogitri/dawarich) uses separate keys: `dawarich.resources` and `sidekiq.resources`
  - Both deployments render correctly: dawarich (10m, 640Mi), sidekiq (10m, 384Mi)
  - Chart follows standard Helm pattern with top-level component keys
  - Verified with `kustomize build --enable-helm` that both deployments have resource requests

## Task - resources-loki
- Added resource requests to loki and promtail pods

## Task - resources-couchdb
- Added resource requests to couchdb pod
- **Learnings:**
  - couchdb uses helm chart couchdb-4.6.3
  - Added `resources.requests` top-level key in parent values.yaml
  - Renders as StatefulSet with 20m CPU, 256Mi memory
  - Verified with `kustomize build --enable-helm` that resource requests render correctly

## Task - resources-wallabag
- Added resource requests to wallabag deployment and cronjob
- Files changed:
  - `/home/devin/Projects/homelab/charts/wallabag/values.yaml:4-7` (deployment)
  - `/home/devin/Projects/homelab/charts/wallabag/tagger.yaml:20-23` (cronjob)
- **Learnings:**
  - wallabag uses helm chart wallabag-0.1.1 for main deployment
  - wallabag-tagger is a separate raw CronJob manifest (runs daily at 1am)
  - Deployment: 10m CPU, 256Mi memory
  - CronJob: 10m CPU, 128Mi memory
  - Verified both resources with `kustomize build --enable-helm | kubectl apply --dry-run=client`

## Task - resources-cert-manager
- Added resource requests to all 3 cert-manager namespace pods via values.yaml
- Files changed: `/home/devin/Projects/homelab/charts/cert-manager/values.yaml:11-23`
- **Learnings:**
  - cert-manager v1.19.2 chart has 3 deployments: controller, webhook, cainjector
  - Top-level `resources:` key controls controller (10m CPU, 256Mi memory)
  - `webhook.resources:` key controls webhook (10m CPU, 128Mi memory)
  - `cainjector.resources:` key controls cainjector (10m CPU, 256Mi memory)
  - Verified with `kustomize build --enable-helm` and `kubectl apply --dry-run=client`

## Task - resources-cloudflared
- Added resource requests to cloudflared Deployment (2 replicas)
- Files changed: `/home/devin/Projects/homelab/charts/cloudflared/cloudflared.yaml:41-44`
- **Learnings:**
  - cloudflared is a raw Deployment manifest (not Helm)
  - Has 2 replicas for HA
  - Each pod: 10m CPU, 128Mi memory
  - Verified with `kubectl apply --dry-run=client`

## Task - resources-external-dns
- Added resource requests to external-dns Deployment via values.yaml
- Files changed: `/home/devin/Projects/homelab/charts/external-dns/values.yaml:18-22`
- **Learnings:**
  - external-dns uses helm chart external-dns-1.20.0
  - Added top-level `resources.requests` key in values.yaml
  - Renders as Deployment with 10m CPU, 128Mi memory
  - Verified with `kustomize build --enable-helm` and `kubectl apply --dry-run=client`

## Task - resources-speedtest
- Added resource requests to speedtest Deployment
- Files changed: `/home/devin/Projects/homelab/charts/speedtest/deployment.yaml:19-22`
- **Learnings:**
  - speedtest is a raw Deployment manifest (not Helm)
  - Added `resources.requests` directly under container spec
  - Renders as Deployment with 10m CPU, 128Mi memory
  - Verified with `kubectl apply --dry-run=client`

## Task - resources-kube-system
- Added resource requests to sealed-secrets-controller via Argo CD Application
- Files changed: `/home/devin/Projects/homelab/apps/templates/sealedsecrets.yaml:20-23`
- **Learnings:**
  - sealed-secrets is deployed via Argo CD Application (not direct Helm)
  - Uses Bitnami sealed-secrets chart version 2.7.3
  - Resource requests added to `helm.values` section in ArgoCD Application manifest
  - Renders as Deployment with 10m CPU, 128Mi memory
  - Verified with `kubectl apply --dry-run=client`

## Task - resources-homepage
- Added resource requests to homepage Deployment
- Files changed: `/home/devin/Projects/homelab/charts/homepage/deployment.yaml:32-35`
- **Learnings:**
  - homepage is a raw Deployment manifest (not Helm)
  - Added `resources.requests` directly under container spec
  - Renders as Deployment with 10m CPU, 256Mi memory
  - Verified with `kubectl apply --dry-run=client`

## Task - resources-umami
- Added resource requests to umami Deployment
- Files changed: `/home/devin/Projects/homelab/charts/umami/manifest.yaml:31-34`
- **Learnings:**
  - umami is a raw Deployment manifest (not Helm)
  - Added `resources.requests` directly under container spec
  - Renders as Deployment with 10m CPU, 384Mi memory
  - Verified with `kubectl apply --dry-run=client` and `kustomize build`

## Task - resources-btc-tracker
- Added resource requests to btc-tracker Deployment
- Files changed: `/home/devin/Projects/homelab/charts/btc-tracker/service.yaml:86-89`
- **Learnings:**
  - btc-tracker is a raw Deployment manifest included in service.yaml (not Helm)
  - Deployment, Service, and HTTPRoute all defined in single service.yaml file
  - Added `resources.requests` directly under container spec
  - Renders as Deployment with 10m CPU, 256Mi memory
  - Verified with `kubectl apply --dry-run=client` and `kustomize build`

## Task - resources-sure
- Added resource requests to both sure namespace deployments
- Files changed: `/home/devin/Projects/homelab/charts/sure/deployment.yaml:41-44,79-82`
- **Learnings:**
  - sure is a raw manifest with 2 Deployments in single file (not Helm)
  - sure deployment: main Rails app (10m CPU, 640Mi memory)
  - sure-worker deployment: Sidekiq worker process (10m CPU, 640Mi memory)
  - Both share same PVC for /rails/storage
  - Verified with `kubectl apply --dry-run=client`

