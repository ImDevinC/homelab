# DO NOT EDIT
# THIS FILE IS GENERATED AUTOMATICALLY AND ANY CHANGES WILL BE OVERWRITTEN
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: generated-rules
spec:
  "groups":
    - "name": "cert-manager"
      "rules":
        - "alert": "CertManagerAbsent"
          "annotations":
            "description": "New certificates will not be able to be minted, and existing ones can't be renewed until cert-manager is back."
            "runbook_url": "https://gitlab.com/uneeq-oss/cert-manager-mixin/-/blob/master/RUNBOOK.md#certmanagerabsent"
            "summary": "Cert Manager has dissapeared from Prometheus service discovery."
          "expr": "absent(up{job=\"cert-manager\"})"
          "for": "10m"
          "labels":
            "severity": "critical"
    - "name": "certificates"
      "rules":
        - "alert": "CertManagerCertExpirySoon"
          "annotations":
            "dashboard_url": "http://grafana.int.imdevinc.com/d/TvuRo2iMk/cert-manager"
            "description": "The domain that this cert covers will be unavailable after {{`{{ $value | humanizeDuration }}`}}. Clients using endpoints that this cert protects will start to fail in {{`{{ $value | humanizeDuration }}`}}."
            "runbook_url": "https://gitlab.com/uneeq-oss/cert-manager-mixin/-/blob/master/RUNBOOK.md#certmanagercertexpirysoon"
            "summary": "The cert `{{`{{ $labels.name }}`}}` is {{`{{ $value | humanizeDuration }}`}} from expiry, it should have renewed over a week ago."
          "expr": |
            avg by (exported_namespace, namespace, name) (
              certmanager_certificate_expiration_timestamp_seconds - time()
            ) < (21 * 24 * 3600) # 21 days in seconds
          "for": "1h"
          "labels":
            "severity": "warning"
        - "alert": "CertManagerCertNotReady"
          "annotations":
            "dashboard_url": "http://grafana.int.imdevinc.com/d/TvuRo2iMk/cert-manager"
            "description": "This certificate has not been ready to serve traffic for at least 10m. If the cert is being renewed or there is another valid cert, the ingress controller _may_ be able to serve that instead."
            "runbook_url": "https://gitlab.com/uneeq-oss/cert-manager-mixin/-/blob/master/RUNBOOK.md#certmanagercertnotready"
            "summary": "The cert `{{`{{ $labels.name }}`}}` is not ready to serve traffic."
          "expr": |
            max by (name, exported_namespace, namespace, condition) (
              certmanager_certificate_ready_status{condition!="True"} == 1
            )
          "for": "10m"
          "labels":
            "severity": "critical"
        - "alert": "CertManagerHittingRateLimits"
          "annotations":
            "dashboard_url": "http://grafana.int.imdevinc.com/d/TvuRo2iMk/cert-manager"
            "description": "Depending on the rate limit, cert-manager may be unable to generate certificates for up to a week."
            "runbook_url": "https://gitlab.com/uneeq-oss/cert-manager-mixin/-/blob/master/RUNBOOK.md#certmanagerhittingratelimits"
            "summary": "Cert manager hitting LetsEncrypt rate limits."
          "expr": |
            sum by (host) (
              rate(certmanager_http_acme_client_request_count{status="429"}[5m])
            ) > 0
          "for": "5m"
          "labels":
            "severity": "critical"
    - "name": "homeassistant"
      "rules":
        - "alert": "SenseEnergyMissing"
          "annotations":
            "description": "Sense energy data is missing"
            "runbook_url": "https://homeassistant.imdevinc.com"
            "summary": "Sense energy data is missing"
          "expr": |
            absent(homeassistant_sensor_power_w{entity=~"sensor.energy_production|sensor.energy_usage"}) > 0
          "for": "15m"
          "labels":
            "app": "sense"
            "severity": "warning"
    - "name": "KubestateExporter"
      "rules":
        - "alert": "KubernetesNodeNotReady"
          "annotations":
            "description": |
              Node {{`{{ $labels.node }}`}} has been unready for a long time
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Node ready (node {{`{{ $labels.node }}`}})"
          "expr": "kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0"
          "for": "10m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesNodeSchedulingDisabled"
          "annotations":
            "description": |
              Node {{`{{ $labels.node }}`}} has been marked as unschedulable for more than 30 minutes.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes node scheduling disabled (node {{`{{ $labels.node }}`}})"
          "expr": "kube_node_spec_taint{key=\"node.kubernetes.io/unschedulable\"} == 1"
          "for": "30m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesNodeMemoryPressure"
          "annotations":
            "description": |
              Node {{`{{ $labels.node }}`}} has MemoryPressure condition
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes memory pressure (node {{`{{ $labels.node }}`}})"
          "expr": "kube_node_status_condition{condition=\"MemoryPressure\",status=\"true\"} == 1"
          "for": "2m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesNodeDiskPressure"
          "annotations":
            "description": |
              Node {{`{{ $labels.node }}`}} has DiskPressure condition
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes disk pressure (node {{`{{ $labels.node }}`}})"
          "expr": "kube_node_status_condition{condition=\"DiskPressure\",status=\"true\"} == 1"
          "for": "2m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesNodeNetworkUnavailable"
          "annotations":
            "description": |
              Node {{`{{ $labels.node }}`}} has NetworkUnavailable condition
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Node network unavailable (instance {{`{{ $labels.instance }}`}})"
          "expr": "kube_node_status_condition{condition=\"NetworkUnavailable\",status=\"true\"} == 1"
          "for": "2m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesNodeOutOfPodCapacity"
          "annotations":
            "description": |
              Node {{`{{ $labels.node }}`}} is out of pod capacity
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Node out of pod capacity (instance {{`{{ $labels.instance }}`}})"
          "expr": "sum by (node) ((kube_pod_status_phase{phase=\"Running\"} == 1) + on(uid, instance) group_left(node) (0 * kube_pod_info{pod_template_hash=\"\"})) / sum by (node) (kube_node_status_allocatable{resource=\"pods\"}) * 100 > 90"
          "for": "2m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesContainerOomKiller"
          "annotations":
            "description": |
              Container {{`{{ $labels.container }}`}} in pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been OOMKilled {{`{{ $value }}`}} times in the last 10 minutes.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes container oom killer ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}}:{{`{{ $labels.container }}`}})"
          "expr": "(kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1"
          "for": "0m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesJobFailed"
          "annotations":
            "description": |
              Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} failed to complete
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Job failed ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}})"
          "expr": "kube_job_status_failed > 0"
          "for": "0m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesJobNotStarting"
          "annotations":
            "description": |
              Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} did not start for 10 minutes
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Job not starting ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}})"
          "expr": "kube_job_status_active == 0 and kube_job_status_failed == 0 and kube_job_status_succeeded == 0 and (time() - kube_job_status_start_time) > 600"
          "for": "0m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesCronjobSuspended"
          "annotations":
            "description": |
              CronJob {{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}} is suspended
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes CronJob suspended ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}})"
          "expr": "kube_cronjob_spec_suspend != 0"
          "for": "0m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesPersistentvolumeclaimPending"
          "annotations":
            "description": |
              PersistentVolumeClaim {{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}} is pending
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes PersistentVolumeClaim pending ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}})"
          "expr": "kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1"
          "for": "2m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesVolumeOutOfDiskSpace"
          "annotations":
            "description": |
              Volume is almost full (< 10% left)
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Volume out of disk space (instance {{`{{ $labels.instance }}`}})"
          "expr": "kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10"
          "for": "2m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesVolumeFullInFourDays"
          "annotations":
            "description": |
              Volume under {{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}} is expected to fill up within four days. Currently {{`{{ $value | humanize }}`}}% is available.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Volume full in four days (instance {{`{{ $labels.instance }}`}})"
          "expr": "predict_linear(kubelet_volume_stats_available_bytes[6h:5m], 4 * 24 * 3600) < 0"
          "for": "0m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesPersistentvolumeError"
          "annotations":
            "description": |
              Persistent volume {{`{{ $labels.persistentvolume }}`}} is in bad state
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes PersistentVolumeClaim pending ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}})"
          "expr": "kube_persistentvolume_status_phase{phase=~\"Failed|Pending\", job=\"kube-state-metrics\"} > 0"
          "for": "0m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesStatefulsetDown"
          "annotations":
            "description": |
              StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} went down
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes StatefulSet down ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}})"
          "expr": "kube_statefulset_replicas != kube_statefulset_status_replicas_ready > 0"
          "for": "1m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesHpaScaleInability"
          "annotations":
            "description": |
              HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} is unable to scale
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes HPA scale inability (instance {{`{{ $labels.instance }}`}})"
          "expr": "(kube_horizontalpodautoscaler_spec_max_replicas - kube_horizontalpodautoscaler_status_desired_replicas) * on (horizontalpodautoscaler,namespace) (kube_horizontalpodautoscaler_status_condition{condition=\"ScalingLimited\", status=\"true\"} == 1) == 0"
          "for": "2m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesHpaMetricsUnavailability"
          "annotations":
            "description": |
              HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} is unable to collect metrics
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes HPA metrics unavailability (instance {{`{{ $labels.instance }}`}})"
          "expr": "kube_horizontalpodautoscaler_status_condition{status=\"false\", condition=\"ScalingActive\"} == 1"
          "for": "0m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesHpaScaleMaximum"
          "annotations":
            "description": |
              HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} has hit maximum number of desired pods
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes HPA scale maximum (instance {{`{{ $labels.instance }}`}})"
          "expr": "(kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas) and (kube_horizontalpodautoscaler_spec_max_replicas > 1) and (kube_horizontalpodautoscaler_spec_min_replicas != kube_horizontalpodautoscaler_spec_max_replicas)"
          "for": "2m"
          "labels":
            "severity": "info"
        - "alert": "KubernetesHpaUnderutilized"
          "annotations":
            "description": |
              HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} is constantly at minimum replicas for 50% of the time. Potential cost saving here.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes HPA underutilized (instance {{`{{ $labels.instance }}`}})"
          "expr": "max(quantile_over_time(0.5, kube_horizontalpodautoscaler_status_desired_replicas[1d]) == kube_horizontalpodautoscaler_spec_min_replicas) by (horizontalpodautoscaler) > 3"
          "for": "0m"
          "labels":
            "severity": "info"
        - "alert": "KubernetesPodNotHealthy"
          "annotations":
            "description": |
              Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-running state for longer than 15 minutes.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Pod not healthy ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}})"
          "expr": "sum by (namespace, pod) (kube_pod_status_phase{phase=~\"Pending|Unknown|Failed\"}) > 0"
          "for": "15m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesPodCrashLooping"
          "annotations":
            "description": |
              Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is crash looping
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes pod crash looping ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}})"
          "expr": "increase(kube_pod_container_status_restarts_total[1m]) > 3"
          "for": "2m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesReplicasetReplicasMismatch"
          "annotations":
            "description": |
              ReplicaSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.replicaset }}`}} replicas mismatch
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes ReplicasSet mismatch ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.replicaset }}`}})"
          "expr": "kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas"
          "for": "10m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesDeploymentReplicasMismatch"
          "annotations":
            "description": |
              Deployment {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} replicas mismatch
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Deployment replicas mismatch ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}})"
          "expr": "kube_deployment_spec_replicas != kube_deployment_status_replicas_available"
          "for": "10m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesStatefulsetReplicasMismatch"
          "annotations":
            "description": |
              StatefulSet does not match the expected number of replicas.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes StatefulSet replicas mismatch (instance {{`{{ $labels.instance }}`}})"
          "expr": "kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas"
          "for": "10m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesDeploymentGenerationMismatch"
          "annotations":
            "description": |
              Deployment {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} has failed but has not been rolled back.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes Deployment generation mismatch ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}})"
          "expr": "kube_deployment_status_observed_generation != kube_deployment_metadata_generation"
          "for": "10m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesStatefulsetGenerationMismatch"
          "annotations":
            "description": |
              StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} has failed but has not been rolled back.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes StatefulSet generation mismatch ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}})"
          "expr": "kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation"
          "for": "10m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesStatefulsetUpdateNotRolledOut"
          "annotations":
            "description": |
              StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} update has not been rolled out.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes StatefulSet update not rolled out ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}})"
          "expr": "max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)"
          "for": "10m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesDaemonsetRolloutStuck"
          "annotations":
            "description": |
              Some Pods of DaemonSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset }}`}} are not scheduled or not ready
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes DaemonSet rollout stuck ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset }}`}})"
          "expr": "kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0"
          "for": "10m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesDaemonsetMisscheduled"
          "annotations":
            "description": |
              Some Pods of DaemonSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset }}`}} are running where they are not supposed to run
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes DaemonSet misscheduled ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.daemonset }}`}})"
          "expr": "kube_daemonset_status_number_misscheduled > 0"
          "for": "1m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesCronjobTooLong"
          "annotations":
            "description": |
              CronJob {{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}} is taking more than 1h to complete.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes CronJob too long ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}})"
          "expr": "time() - kube_cronjob_next_schedule_time > 3600"
          "for": "0m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesJobSlowCompletion"
          "annotations":
            "description": |
              Kubernetes Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} did not complete in time.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes job slow completion ({{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}})"
          "expr": "kube_job_spec_completions - kube_job_status_succeeded - kube_job_status_failed > 0"
          "for": "12h"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesApiServerErrors"
          "annotations":
            "description": |
              Kubernetes API server is experiencing high error rate
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes API server errors (instance {{`{{ $labels.instance }}`}})"
          "expr": "sum(rate(apiserver_request_total{job=\"apiserver\",code=~\"(?:5..)\"}[1m])) by (instance, job) / sum(rate(apiserver_request_total{job=\"apiserver\"}[1m])) by (instance, job) * 100 > 3"
          "for": "2m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesApiClientErrors"
          "annotations":
            "description": |
              Kubernetes API client is experiencing high error rate
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes API client errors (instance {{`{{ $labels.instance }}`}})"
          "expr": "(sum(rate(rest_client_requests_total{code=~\"(4|5)..\"}[1m])) by (instance, job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) * 100 > 1"
          "for": "2m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesClientCertificateExpiresNextWeek"
          "annotations":
            "description": |
              A client certificate used to authenticate to the apiserver is expiring next week.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes client certificate expires next week (instance {{`{{ $labels.instance }}`}})"
          "expr": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 7*24*60*60"
          "for": "0m"
          "labels":
            "severity": "warning"
        - "alert": "KubernetesClientCertificateExpiresSoon"
          "annotations":
            "description": |
              A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes client certificate expires soon (instance {{`{{ $labels.instance }}`}})"
          "expr": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 24*60*60"
          "for": "0m"
          "labels":
            "severity": "critical"
        - "alert": "KubernetesApiServerLatency"
          "annotations":
            "description": |
              Kubernetes API server has a 99th percentile latency of {{`{{ $value }}`}} seconds for {{`{{ $labels.verb }}`}} {{`{{ $labels.resource }}`}}.
                VALUE = {{`{{ $value }}`}}
                LABELS = {{`{{ $labels }}`}}
            "summary": "Kubernetes API server latency (instance {{`{{ $labels.instance }}`}})"
          "expr": "histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb!~\"(?:CONNECT|WATCHLIST|WATCH|PROXY)\"} [10m])) WITHOUT (subresource)) > 1"
          "for": "2m"
          "labels":
            "severity": "warning"
    - "name": "radarr"
      "rules":
        - "alert": "RadarrStatusOffline"
          "annotations":
            "description": "Radarr system status is reporting as offline"
            "runbook_url": "https://github.com/imdevinc/homelab"
            "summary": "Radarr is currently offline"
          "expr": |
            max_over_time(radarr_system_status[5m]) < 1
          "for": "5m"
          "labels":
            "app": "radarr"
            "severity": "warning"
    - "name": "sonarr"
      "rules":
        - "alert": "SonarrStatusOffline"
          "annotations":
            "description": "Sonarr system status is reporting as offline"
            "runbook_url": "https://github.com/imdevinc/homelab"
            "summary": "Sonarr is currently offline"
          "expr": |
            max_over_time(sonarr_system_status[5m]) < 1
          "for": "5m"
          "labels":
            "app": "sonarr"
            "severity": "warning"
        - "alert": "SonarrDownloadWarning"
          "annotations":
            "description": "There is a warning with one of the downloads in Sonarr"
            "runbook_url": "https://github.com/imdevinc/homelab"
            "summary": "Sonarr is having an issue downloading"
          "expr": |
            max_over_time(sonarr_queue_total{download_status="warning"}[5m]) > 0
          "for": "5m"
          "labels":
            "app": "sonarr"
            "severity": "warning"
    - "name": "PostgreSQL"
      "rules":
        - "alert": "PostgreSQLMaxConnectionsReached"
          "annotations":
            "description": "{{`{{ $labels.instance }}`}} is exceeding the currently configured maximum Postgres connection limit (current value: {{`{{ $value }}`}}s). Services may be degraded - please take immediate action (you probably need to increase max_connections in the Docker image and re-deploy."
            "summary": "{{`{{ $labels.instance }}`}} has maxed out Postgres connections."
          "expr": |
            sum by (instance) (pg_stat_activity_count{})
            >=
            sum by (instance) (pg_settings_max_connections{})
            -
            sum by (instance) (pg_settings_superuser_reserved_connections{})
          "for": "1m"
          "labels":
            "severity": "warning"
        - "alert": "PostgreSQLHighConnections"
          "annotations":
            "description": "{{`{{ $labels.instance }}`}} is exceeding 80% of the currently configured maximum Postgres connection limit (current value: {{`{{ $value }}`}}s). Please check utilization graphs and confirm if this is normal service growth, abuse or an otherwise temporary condition or if new resources need to be provisioned (or the limits increased, which is mostly likely)."
            "summary": "{{`{{ $labels.instance }}`}} is over 80% of max Postgres connections."
          "expr": |
            sum by (instance) (pg_stat_activity_count{})
            >
            (
              sum by (instance) (pg_settings_max_connections{})
              -
              sum by (instance) (pg_settings_superuser_reserved_connections{})
            ) * 0.8
          "for": "10m"
          "labels":
            "severity": "warning"
        - "alert": "PostgreSQLDown"
          "annotations":
            "description": "{{`{{ $labels.instance }}`}} is rejecting query requests from the exporter, and thus probably not allowing DNS requests to work either. User services should not be effected provided at least 1 node is still alive."
            "summary": "PostgreSQL is not processing queries: {{`{{ $labels.instance }}`}}"
          "expr": "pg_up{} != 1"
          "for": "1m"
          "labels":
            "severity": "warning"
        - "alert": "PostgreSQLSlowQueries"
          "annotations":
            "description": "PostgreSQL high number of slow queries {{`{{ $labels.cluster }}`}} for database {{`{{ $labels.datname }}`}} with a value of {{`{{ $value }}`}} "
            "summary": "PostgreSQL high number of slow on {{`{{ $labels.cluster }}`}} for database {{`{{ $labels.datname }}`}} "
          "expr": |
            avg by (datname) (
              rate (
                pg_stat_activity_max_tx_duration{datname!~"template.*",}[2m]
              )
            ) > 2 * 60
          "for": "2m"
          "labels":
            "severity": "warning"
        - "alert": "PostgreSQLQPS"
          "annotations":
            "description": "PostgreSQL high number of queries per second on {{`{{ $labels.cluster }}`}} for database {{`{{ $labels.datname }}`}} with a value of {{`{{ $value }}`}}"
            "summary": "PostgreSQL high number of queries per second {{`{{ $labels.cluster }}`}} for database {{`{{ $labels.datname }}`}}"
          "expr": |
            avg by (datname) (
              irate(
                pg_stat_database_xact_commit{datname!~"template.*",}[5m]
              )
              +
              irate(
                pg_stat_database_xact_rollback{datname!~"template.*",}[5m]
              )
            ) > 10000
          "for": "5m"
          "labels":
            "severity": "warning"
        - "alert": "PostgreSQLCacheHitRatio"
          "annotations":
            "description": "PostgreSQL low on cache hit rate on {{`{{ $labels.cluster }}`}} for database {{`{{ $labels.datname }}`}} with a value of {{`{{ $value }}`}}"
            "summary": "PostgreSQL low cache hit rate on {{`{{ $labels.cluster }}`}} for database {{`{{ $labels.datname }}`}}"
          "expr": |
            avg by (datname) (
              rate(pg_stat_database_blks_hit{datname!~"template.*",}[5m])
              /
              (
                rate(
                  pg_stat_database_blks_hit{datname!~"template.*",}[5m]
                )
                +
                rate(
                  pg_stat_database_blks_read{datname!~"template.*",}[5m]
                )
              )
            ) < 0.98
          "for": "5m"
          "labels":
            "severity": "warning"
    - "name": "redis"
      "rules":
        - "alert": "RedisDown"
          "annotations":
            "description": "Redis instance is down\n  VALUE = {{`{{ $value }}`}}\n  LABELS: {{`{{ $labels }}`}}"
            "summary": "Redis down (instance {{`{{ $labels.instance }}`}})"
          "expr": "redis_up{job=\"redis\"} == 0"
          "for": "5m"
          "labels":
            "severity": "critical"
        - "alert": "RedisOutOfMemory"
          "annotations":
            "description": "Redis is running out of memory (> 90%)\n  VALUE = {{`{{ $value }}`}}\n  LABELS: {{`{{ $labels }}`}}"
            "summary": "Redis out of memory (instance {{`{{ $labels.instance }}`}})"
          "expr": "redis_memory_used_bytes{job=\"redis\"} / redis_total_system_memory_bytes{job=\"redis\"} * 100 > 90"
          "for": "5m"
          "labels":
            "severity": "warning"
        - "alert": "RedisTooManyConnections"
          "annotations":
            "description": "Redis instance has too many connections\n  VALUE = {{`{{ $value }}`}}\n  LABELS: {{`{{ $labels }}`}}"
            "summary": "Redis too many connections (instance {{`{{ $labels.instance }}`}})"
          "expr": "redis_connected_clients{job=\"redis\"} > 100"
          "for": "5m"
          "labels":
            "severity": "warning"
        - "alert": "RedisClusterSlotFail"
          "annotations":
            "description": "Redis cluster has slots fail\n  VALUE = {{`{{ $value }}`}}\n  LABELS: {{`{{ $labels }}`}}"
            "summary": "Number of hash slots mapping to a node in FAIL state (instance {{`{{ $labels.instance }}`}})"
          "expr": "redis_cluster_slots_fail{job=\"redis\"} > 0"
          "for": "5m"
          "labels":
            "severity": "warning"
        - "alert": "RedisClusterSlotPfail"
          "annotations":
            "description": "Redis cluster has slots pfail\n  VALUE = {{`{{ $value }}`}}\n  LABELS: {{`{{ $labels }}`}}"
            "summary": "Number of hash slots mapping to a node in PFAIL state (instance {{`{{ $labels.instance }}`}})"
          "expr": "redis_cluster_slots_pfail{job=\"redis\"} > 0"
          "for": "5m"
          "labels":
            "severity": "warning"
        - "alert": "RedisClusterStateNotOk"
          "annotations":
            "description": "Redis cluster is not ok\n  VALUE = {{`{{ $value }}`}}\n  LABELS: {{`{{ $labels }}`}}"
            "summary": "Redis cluster state is not ok (instance {{`{{ $labels.instance }}`}})"
          "expr": "redis_cluster_state{job=\"redis\"} == 0"
          "for": "5m"
          "labels":
            "severity": "critical"
    - "name": "sealed-secrets"
      "rules":
        - "alert": "SealedSecretsUnsealErrorHigh"
          "annotations":
            "description": "High number of errors during unsealing Sealed Secrets in {{`{{ $labels.namespace }}`}} namespace."
            "runbook_url": "https://github.com/bitnami-labs/sealed-secrets"
            "summary": "Sealed Secrets Unseal Error High"
          "expr": |
            sum by (reason, namespace) (rate(sealed_secrets_controller_unseal_errors_total{}[5m])) > 0
          "labels":
            "severity": "warning"
