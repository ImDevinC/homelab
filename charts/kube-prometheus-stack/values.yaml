kube-prometheus-stack:
  defaultRules:
    disabled:
      etcdDatabaseHighFragmentationRatio: true
  alertmanager:
    ingress:
      enabled: true
      hosts:
        - alertmanager.collins.home
      ingressClassName: nginx
    alertmanagerSpec:
      externalUrl: http://alertmanager.collins.home
      useExistingSecret: true
      configSecret: alertmanager-new-config
      forceEnableClusterMode: true
  grafana:
    sidecar:
      dashboards:
        enabled: true
    plugins:
      - camptocamp-prometheus-alertmanager-datasource
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Loki
            type: loki
            url: http://loki.loki:3100
            access: proxy
    ingress:
      enabled: true
      hosts:
        - grafana.collins.home
      ingressClassName: nginx
      annotations:
        nginx.org/proxy-read-timeout: "3600"
        nginx.org/proxy-send-timeout: "3600"
        nginx.org/websocket-services: kube-prometheus-stack-grafana
    admin:
      existingSecret: grafana-creds
    extraSecretMounts:
      - name: grafana-authentik-creds
        secretName: grafana-authentik-creds
        defaultMode: 0440
        mountPath: /etc/secrets/grafana_authentik_secret
        readOnly: true
    grafana.ini:
      server:
        root_url: http://grafana.collins.home
      auth.generic_oauth:
        enabled: true
        name: Authentik SSO
        allow_sign_up: true
        client_id: "$__file{/etc/secrets/grafana_authentik_secret/client_id}"
        client_secret: "$__file{/etc/secrets/grafana_authentik_secret/client_secret}"
        auto_login: true
        scopes: openid profile email
        api_url: http://authentik.collins.home/application/o/userinfo/
        auth_url: http://authentik.collins.home/application/o/authorize/
        token_url: http://authentik.collins.home/application/o/token/
        role_attribute_path: "contains(groups, 'Grafana Admins') && 'Admin' || contains(groups, 'Grafana Editors') && 'Editor' || 'Viewer'"
  prometheus:
    ingress:
      enabled: true
      ingressClassName: nginx
      hosts:
        - prometheus.collins.home
    prometheusSpec:
      additionalScrapeConfigs:
        - job_name: node-exporter
          static_configs:
            - targets: ['archlinux:9100']
            - targets: ['pi.hole:9100']
        - job_name: nvidia-smi
          static_configs:
            - targets: ['archlinux:9835']
        - job_name: rpi
          static_configs:
            - targets: ['pi.hole:9110']
      serviceMonitorSelectorNilUsesHelmValues: false
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: hostpath-csi
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 50Gi
  additionalPrometheusRulesMap:
    alerts:
      "groups":
        - "name": "cert-manager"
          "rules":
            - "alert": "CertManagerAbsent"
              "annotations":
                "description": "New certificates will not be able to be minted, and existing ones can't be renewed until cert-manager is back."
                "runbook_url": "https://gitlab.com/uneeq-oss/cert-manager-mixin/-/blob/master/RUNBOOK.md#certmanagerabsent"
                "summary": "Cert Manager has dissapeared from Prometheus service discovery."
              "expr": "absent(up{job=\"cert-manager\"})"
              "for": "10m"
              "labels":
                "severity": "critical"
        - "name": "certificates"
          "rules":
            - "alert": "CertManagerCertExpirySoon"
              "annotations":
                "dashboard_url": "http://grafana.collins.home/d/TvuRo2iMk/cert-manager"
                "description": "The domain that this cert covers will be unavailable after {{ $value | humanizeDuration }}. Clients using endpoints that this cert protects will start to fail in {{ $value | humanizeDuration }}."
                "runbook_url": "https://gitlab.com/uneeq-oss/cert-manager-mixin/-/blob/master/RUNBOOK.md#certmanagercertexpirysoon"
                "summary": "The cert `{{ $labels.name }}` is {{ $value | humanizeDuration }} from expiry, it should have renewed over a week ago."
              "expr": |
                avg by (exported_namespace, namespace, name) (
                  certmanager_certificate_expiration_timestamp_seconds - time()
                ) < (21 * 24 * 3600) # 21 days in seconds
              "for": "1h"
              "labels":
                "severity": "warning"
            - "alert": "CertManagerCertNotReady"
              "annotations":
                "dashboard_url": "http://grafana.collins.home/d/TvuRo2iMk/cert-manager"
                "description": "This certificate has not been ready to serve traffic for at least 10m. If the cert is being renewed or there is another valid cert, the ingress controller _may_ be able to serve that instead."
                "runbook_url": "https://gitlab.com/uneeq-oss/cert-manager-mixin/-/blob/master/RUNBOOK.md#certmanagercertnotready"
                "summary": "The cert `{{ $labels.name }}` is not ready to serve traffic."
              "expr": |
                max by (name, exported_namespace, namespace, condition) (
                  certmanager_certificate_ready_status{condition!="True"} == 1
                )
              "for": "10m"
              "labels":
                "severity": "critical"
            - "alert": "CertManagerHittingRateLimits"
              "annotations":
                "dashboard_url": "http://grafana.collins.home/d/TvuRo2iMk/cert-manager"
                "description": "Depending on the rate limit, cert-manager may be unable to generate certificates for up to a week."
                "runbook_url": "https://gitlab.com/uneeq-oss/cert-manager-mixin/-/blob/master/RUNBOOK.md#certmanagerhittingratelimits"
                "summary": "Cert manager hitting LetsEncrypt rate limits."
              "expr": |
                sum by (host) (
                  rate(certmanager_http_acme_client_request_count{status="429"}[5m])
                ) > 0
              "for": "5m"
              "labels":
                "severity": "critical"
        - "name": "sealed-secrets"
          "rules":
            - "alert": "SealedSecretsUnsealErrorHigh"
              "annotations":
                "description": "High number of errors during unsealing Sealed Secrets in {{ $labels.namespace }} namespace."
                "runbook_url": "https://github.com/bitnami-labs/sealed-secrets"
                "summary": "Sealed Secrets Unseal Error High"
              "expr": |
                sum by (reason, namespace) (rate(sealed_secrets_controller_unseal_errors_total{}[5m])) > 0
              "labels":
                "severity": "warning"
        - "name": "radarr"
          "rules":
            - "alert": "RadarrStatusOffline"
              "annotations":
                "description": "Radarr system status is reporting as offline"
                "runbook_url": "https://github.com/imdevinc/homelab"
                "summary": "Radarr is currently offline"
              "expr": |
                max_over_time(radarr_system_status[5m]) < 1
              "for": "5m"
              "labels":
                "app": "radarr"
                "severity": "warning"
        - "name": "sonarr"
          "rules":
            - "alert": "SonarrStatusOffline"
              "annotations":
                "description": "Sonarr system status is reporting as offline"
                "runbook_url": "https://github.com/imdevinc/homelab"
                "summary": "Sonarr is currently offline"
              "expr": |
                max_over_time(sonarr_system_status[5m]) < 1
              "for": "5m"
              "labels":
                "app": "sonarr"
                "severity": "warning"
            - "alert": "SonarrDownloadWarning"
              "annotations":
                "description": "There is a warning with one of the downloads in Sonarr"
                "runbook_url": "https://github.com/imdevinc/homelab"
                "summary": "Sonarr is having an issue downloading"
              "expr": |
                max_over_time(sonarr_queue_total{download_status="warning"}[5m]) > 0
              "for": "5m"
              "labels":
                "app": "sonarr"
                "severity": "warning"
        - "name": "homeassistant"
          "rules":
            - "alert": "SenseEnergyMissing"
              "annotations":
                "description": "Sense energy data is missing"
                "runbook_url": "https://homeassistant.imdevinc.com"
                "summary": "Sense energy data is missing"
              "expr": |
                absent(homeassistant_sensor_power_w{entity=~"sensor.energy_production|sensor.energy_usage"}) > 0
              "for": "15m"
              "labels":
                "app": "sense"
                "severity": "warning"
        - "name": "PostgreSQL"
          "rules":
            - "alert": "PostgreSQLMaxConnectionsReached"
              "annotations":
                "description": "{{ $labels.instance }} is exceeding the currently configured maximum Postgres connection limit (current value: {{ $value }}s). Services may be degraded - please take immediate action (you probably need to increase max_connections in the Docker image and re-deploy."
                "summary": "{{ $labels.instance }} has maxed out Postgres connections."
              "expr": |
                sum by (instance) (pg_stat_activity_count{})
                >=
                sum by (instance) (pg_settings_max_connections{})
                -
                sum by (instance) (pg_settings_superuser_reserved_connections{})
              "for": "1m"
              "labels":
                "severity": "warning"
            - "alert": "PostgreSQLHighConnections"
              "annotations":
                "description": "{{ $labels.instance }} is exceeding 80% of the currently configured maximum Postgres connection limit (current value: {{ $value }}s). Please check utilization graphs and confirm if this is normal service growth, abuse or an otherwise temporary condition or if new resources need to be provisioned (or the limits increased, which is mostly likely)."
                "summary": "{{ $labels.instance }} is over 80% of max Postgres connections."
              "expr": |
                sum by (instance) (pg_stat_activity_count{})
                >
                (
                  sum by (instance) (pg_settings_max_connections{})
                  -
                  sum by (instance) (pg_settings_superuser_reserved_connections{})
                ) * 0.8
              "for": "10m"
              "labels":
                "severity": "warning"
            - "alert": "PostgreSQLDown"
              "annotations":
                "description": "{{ $labels.instance }} is rejecting query requests from the exporter, and thus probably not allowing DNS requests to work either. User services should not be effected provided at least 1 node is still alive."
                "summary": "PostgreSQL is not processing queries: {{ $labels.instance }}"
              "expr": "pg_up{} != 1"
              "for": "1m"
              "labels":
                "severity": "warning"
            - "alert": "PostgreSQLSlowQueries"
              "annotations":
                "description": "PostgreSQL high number of slow queries {{ $labels.cluster }} for database {{ $labels.datname }} with a value of {{ $value }} "
                "summary": "PostgreSQL high number of slow on {{ $labels.cluster }} for database {{ $labels.datname }} "
              "expr": |
                avg by (datname) (
                  rate (
                    pg_stat_activity_max_tx_duration{datname!~"template.*",}[2m]
                  )
                ) > 2 * 60
              "for": "2m"
              "labels":
                "severity": "warning"
            - "alert": "PostgreSQLQPS"
              "annotations":
                "description": "PostgreSQL high number of queries per second on {{ $labels.cluster }} for database {{ $labels.datname }} with a value of {{ $value }}"
                "summary": "PostgreSQL high number of queries per second {{ $labels.cluster }} for database {{ $labels.datname }}"
              "expr": |
                avg by (datname) (
                  irate(
                    pg_stat_database_xact_commit{datname!~"template.*",}[5m]
                  )
                  +
                  irate(
                    pg_stat_database_xact_rollback{datname!~"template.*",}[5m]
                  )
                ) > 10000
              "for": "5m"
              "labels":
                "severity": "warning"
            - "alert": "PostgreSQLCacheHitRatio"
              "annotations":
                "description": "PostgreSQL low on cache hit rate on {{ $labels.cluster }} for database {{ $labels.datname }} with a value of {{ $value }}"
                "summary": "PostgreSQL low cache hit rate on {{ $labels.cluster }} for database {{ $labels.datname }}"
              "expr": |
                avg by (datname) (
                  rate(pg_stat_database_blks_hit{datname!~"template.*",}[5m])
                  /
                  (
                    rate(
                      pg_stat_database_blks_hit{datname!~"template.*",}[5m]
                    )
                    +
                    rate(
                      pg_stat_database_blks_read{datname!~"template.*",}[5m]
                    )
                  )
                ) < 0.98
              "for": "5m"
              "labels":
                "severity": "warning"
        - "name": "redis"
          "rules":
            - "alert": "RedisDown"
              "annotations":
                "description": "Redis instance is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
                "summary": "Redis down (instance {{ $labels.instance }})"
              "expr": "redis_up{job=\"redis\"} == 0"
              "for": "5m"
              "labels":
                "severity": "critical"
            - "alert": "RedisOutOfMemory"
              "annotations":
                "description": "Redis is running out of memory (> 90%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
                "summary": "Redis out of memory (instance {{ $labels.instance }})"
              "expr": "redis_memory_used_bytes{job=\"redis\"} / redis_total_system_memory_bytes{job=\"redis\"} * 100 > 90"
              "for": "5m"
              "labels":
                "severity": "warning"
            - "alert": "RedisTooManyConnections"
              "annotations":
                "description": "Redis instance has too many connections\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
                "summary": "Redis too many connections (instance {{ $labels.instance }})"
              "expr": "redis_connected_clients{job=\"redis\"} > 100"
              "for": "5m"
              "labels":
                "severity": "warning"
            - "alert": "RedisClusterSlotFail"
              "annotations":
                "description": "Redis cluster has slots fail\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
                "summary": "Number of hash slots mapping to a node in FAIL state (instance {{ $labels.instance }})"
              "expr": "redis_cluster_slots_fail{job=\"redis\"} > 0"
              "for": "5m"
              "labels":
                "severity": "warning"
            - "alert": "RedisClusterSlotPfail"
              "annotations":
                "description": "Redis cluster has slots pfail\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
                "summary": "Number of hash slots mapping to a node in PFAIL state (instance {{ $labels.instance }})"
              "expr": "redis_cluster_slots_pfail{job=\"redis\"} > 0"
              "for": "5m"
              "labels":
                "severity": "warning"
            - "alert": "RedisClusterStateNotOk"
              "annotations":
                "description": "Redis cluster is not ok\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
                "summary": "Redis cluster state is not ok (instance {{ $labels.instance }})"
              "expr": "redis_cluster_state{job=\"redis\"} == 0"
              "for": "5m"
              "labels":
                "severity": "critical"
